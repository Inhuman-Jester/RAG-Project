{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0cde3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\")\n",
    "\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "LANGSMITH_API_KEY = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "LANGSMITH_PROJECT = os.getenv(\"LANGSMITH_PROJECT\")\n",
    "LANGSMITH_TRACING_V2 = os.getenv(\"LANGSMITH_TRACING_V2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0217da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import utils\n",
    "utils.tracing_is_enabled()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd1a94e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, RunnableSequence\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Initialize Pinecone client\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Index Name\n",
    "index_name = \"testprojectv2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15b4625d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 0c7ac3fd-5029-4338-9f61-c027a611b302)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 0c7ac3fd-5029-4338-9f61-c027a611b302)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "st_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "EMBEDDING_DIM = 384  # all-MiniLM-L6-v2 output size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27918d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_index():\n",
    "    existing_indexes = [index[\"name\"] for index in pc.list_indexes()]\n",
    "    if index_name in existing_indexes:\n",
    "        logging.info(f\"Index '{index_name}' already exists. Skipping creation.\")\n",
    "    else:\n",
    "        logging.info(f\"Creating Pinecone index: {index_name}\")\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=EMBEDDING_DIM,\n",
    "            metric='cosine',\n",
    "            spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
    "        )\n",
    "        time.sleep(5)  # Ensure index is ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b4d082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_documents(filepath):\n",
    "    logging.info(\"Loading document...\")\n",
    "    # Use the filepath passed into the function so this works for any PDF path\n",
    "    loader = PyPDFLoader(filepath)\n",
    "    docs = loader.load()\n",
    "\n",
    "    logging.info(\"Splitting documents into chunks...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        add_start_index=True\n",
    "    )\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    return {\"all_splits\": splits, \"total_Splits:\": len(splits), \"message\": \"Documents loaded and split successfully!\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a010a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "st_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "\n",
    "def embed_documents(inputs):\n",
    "    splits = inputs[\"all_splits\"]\n",
    "    logging.info(\"Generating embeddings (sentence-transformers)...\")\n",
    "    texts = [split.page_content for split in splits]\n",
    "    embeddings = st_model.encode(texts, convert_to_numpy=True)  # shape: (N, EMBEDDING_DIM)\n",
    "    norms = [float(e[:5].sum()) for e in embeddings[:5]]\n",
    "    return {\n",
    "        \"all_embeddings\": embeddings,\n",
    "        \"norms\": norms,\n",
    "        \"message\": \"Embeddings generated successfully with sentence-transformers!\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6d4f823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_embeddings(data):  # This function expects a dictionary\n",
    "    splits = data[\"splits\"][\"all_splits\"]\n",
    "    embeddings = data[\"embeddings\"][\"all_embeddings\"]\n",
    "    logging.info(f\"Upserting {len(embeddings)} documents into Pinecone...\")\n",
    "    index = pc.Index(index_name)\n",
    "\n",
    "    vectors = [\n",
    "        {\n",
    "            # Ensure \"id\" is valid and unique(because vectors of same Id get overwritten by the latest vector)\n",
    "            \"id\": f\"doc_{split.metadata.get('source')}_{i}_{split.metadata.get('page_label', 'no_label')}\",  \n",
    "            \"values\": emb,\n",
    "            \"metadata\": {\"text\": split.page_content}\n",
    "        }\n",
    "        for i, (split, emb) in enumerate(zip(splits, embeddings)) if len(emb) > 0\n",
    "    ]\n",
    "\n",
    "    BATCH_SIZE = 100  # Recommended batch size\n",
    "    for i in range(0, len(vectors), BATCH_SIZE):\n",
    "        batch = vectors[i:i + BATCH_SIZE]\n",
    "        index.upsert(vectors=batch, namespace='ns1')\n",
    "        logging.info(f\"Upserted batch {i // BATCH_SIZE + 1} of {len(vectors) // BATCH_SIZE + 1}\")\n",
    "    \n",
    "    logging.info(f\"Upserted {len(vectors)} vectors into the vector store.\")\n",
    "    return f\"Upserted {len(vectors)} vectors into the vector store.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f2052b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnableSequence\n",
    "\n",
    "# Turn Functions into Runnables\n",
    "load_split_runnable = RunnableLambda(load_and_split_documents)\n",
    "embed_runnable = RunnableLambda(embed_documents)\n",
    "upsert_runnable = RunnableLambda(upsert_embeddings)\n",
    "\n",
    "# Using LangChain's | Operator for an Indexing Chain\n",
    "indexing_chain = (\n",
    "    load_split_runnable \n",
    "    | {\n",
    "        \"splits\": RunnablePassthrough(),\n",
    "        \"embeddings\": embed_runnable\n",
    "    }\n",
    "    | upsert_runnable\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "756fc8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_indexing_pipeline(filepath):\n",
    "    ensure_index()\n",
    "    \n",
    "    indexing_chain.invoke(filepath)\n",
    "    \n",
    "    logging.info(\"Indexing pipeline completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4594aa43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Creating Pinecone index: testprojectv2\n",
      "INFO:root:Loading document...\n",
      "INFO:root:Splitting documents into chunks...\n",
      "INFO:root:Generating embeddings (sentence-transformers)...\n",
      "Batches: 100%|██████████| 2/2 [00:02<00:00,  1.33s/it]\n",
      "INFO:root:Upserting 53 documents into Pinecone...\n",
      "INFO:root:Upserted batch 1 of 1\n",
      "INFO:root:Upserted 53 vectors into the vector store.\n",
      "INFO:root:Indexing pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Call the indexing pipeline with the PDF filename (ensure file exists in the workspace)\n",
    "run_indexing_pipeline(\"Euthyphro.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20c3ff08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_response_info': {'raw_headers': {'connection': 'keep-alive',\n",
      "                                    'content-length': '175',\n",
      "                                    'content-type': 'application/json',\n",
      "                                    'date': 'Sat, 27 Dec 2025 10:33:01 GMT',\n",
      "                                    'grpc-status': '0',\n",
      "                                    'server': 'envoy',\n",
      "                                    'x-envoy-upstream-service-time': '89',\n",
      "                                    'x-pinecone-request-id': '6702328674259901553',\n",
      "                                    'x-pinecone-request-latency-ms': '88',\n",
      "                                    'x-pinecone-response-duration-ms': '90'}},\n",
      " 'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'memoryFullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {'ns1': {'vector_count': 53}},\n",
      " 'storageFullness': 0.0,\n",
      " 'total_vector_count': 53,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "source": [
    "index = pc.Index(index_name)\n",
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468ff0a2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bff6c612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reuse st_model and EMBEDDING_DIM from above\n",
    "\n",
    "def retriever(question):\n",
    "    # encode returns a 1D array, Pinecone expects a list\n",
    "    embedded_question = st_model.encode([question], convert_to_numpy=True)[0].tolist()\n",
    "    similar_docs = index.query(\n",
    "        vector=embedded_question,\n",
    "        top_k=3,\n",
    "        namespace=\"ns1\",\n",
    "        include_metadata=True\n",
    "    )\n",
    "    return similar_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92bdbe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatContext(retrieved_docs):\n",
    "    return \"\\n\".join(doc[\"metadata\"][\"text\"] for doc in retrieved_docs['matches'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf45eb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Wrap retriever and formatContext as runnables\n",
    "retriever_runnable = RunnableLambda(retriever)\n",
    "formatContext_runnable = RunnableLambda(formatContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "550efeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Answer the user question based on the following context.\n",
    "    If you dont know the answer, just say you dont know.\n",
    "                                          \n",
    "    Context: {context} \n",
    "                                          \n",
    "    Question: {question}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97d27871",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=1.0,  # Gemini 3.0+ defaults to 1.0\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5ac9d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputParser(response):\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5ea5da4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever_runnable | formatContext_runnable, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | outputParser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "65c2551d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.63it/s]\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Based on the provided context, the book is a dialogue between Socrates and Euthyphro. It covers:\\n\\n*   A philosophical discussion about the relationship between fear and reverence.\\n*   Socrates' accusation by Meletus for corrupting the youth.\\n*   Euthyphro's personal case of prosecuting his own father for murder and the debate surrounding what is considered holy and unholy in religious matters.\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question\n",
    "rag_chain.invoke(\"What is the book about?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06d566b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
